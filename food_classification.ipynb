{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7cc4c0-2e2f-40b0-bdd9-9caefa70fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5393217e-88ad-4a26-94ca-b551072ee36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_regression(X, y, w, b, lambda_l = 1):\n",
    "    '''\n",
    "    Description: computes the cost for regularized logistic regression\n",
    "    ( the term with the numerator lambda helps to minimize the size of the parameters \n",
    "    (w1, w2,...),\n",
    "    but I use the version where b is not regularized )\n",
    "    Preconditions: X - m x n matrix which contains our data \n",
    "                    y - m size array - target values\n",
    "                    w - n size array - model parameters\n",
    "                    b - real number - free term parameter\n",
    "                    lambda_l - real number - quantifier for regularization\n",
    "    Postconditons: total_cost - real number - the cost of our calculations, which will help\n",
    "    later to build a better model\n",
    "    '''\n",
    "    m = X.shape[0] # number of input data examples\n",
    "    n = X.shape[1] # number of features of each example\n",
    "\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w)+b\n",
    "        z = np.clip( z, -500, 500 )           # protect against overflow\n",
    "        f_wb = 1.0/(1.0+ np.exp(-z))            # the formula of logistic regression model\n",
    "        cost += -y[i] * np.log(f_wb) - (1-y[i]) * np.log(1 - f_wb)\n",
    "    cost /= m\n",
    "\n",
    "    # the regularization part\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += w[j]**2\n",
    "    reg_cost *= lambda_l/(2*m)\n",
    "\n",
    "    # adding the 2 costs\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41575f66-6630-42cb-85a4-b0752bd1b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_l):\n",
    "    '''\n",
    "    Description: computes the partial derivative of the w vector and b scalar used in the gradient descent algorithm \n",
    "    Preconditions: X - m x n matrix which contains our data \n",
    "                    y - m size array - target values\n",
    "                    w - n size array - model parameters\n",
    "                    b - real number - free term parameter\n",
    "                    lambda_l - real number - quantifier for regularization\n",
    "    Postconditons: dj_dw - n array (size of w) and dj_db - scalar\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "    dj_dw= np.zeros((n,))    # null vector of n size\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w)+b\n",
    "        z = np.clip( z, -500, 500 )           # protect against overflow\n",
    "        f_wb = 1.0/(1.0+ np.exp(-z))\n",
    "        dj_db += f_wb - y[i] # forming the derivative of b \n",
    "\n",
    "       # forming the gradient of w - unregularized part for now\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += (f_wb - y[i])*X[i,j]\n",
    "\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    #now add the regularization part for the w feature vector\n",
    "    for j in range(n):\n",
    "        dj_dw[j] += (lambda_l/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a4a34a6-fc70-44a6-a076-0e84cbbde39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply z-score normalization to bring our input data in the same range, so our model will perform faster and better\n",
    "\n",
    "def zscore_normalize_features(X): \n",
    "    # description: computes  X, zcore normalized by column\n",
    "    # pre: X - mxn array\n",
    "    # post: X_norm : normalized data \n",
    "\n",
    "    # find the mean of each feature, by columns\n",
    "    mu = np.mean(X, axis = 0) # nD array\n",
    "    # find the standard deviation of each column\n",
    "    sigma = np.std(X, axis=0) # nD array\n",
    "    # apply the formula of the z-score normalization\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa7e0d97-7df6-4d47-b429-df2be492736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where FOOD INPUT DATA is taken as input\n",
    "\n",
    "'''\n",
    "We have 25 examples of healthy and unhealthy foods.\n",
    "As target values, we have the classes healthy/unhealthy, marked with 1(healthy)/0(unhealthy)       - OUTPUT\n",
    "Each food has 5 features (measured in 100g or ml):  \n",
    " - Total Fat (g)\n",
    " - Saturated Fat (g)\n",
    " - Carbohydrate (g)                                                            - INPUT\n",
    " - Total Sugars (g) \n",
    " - Protein (g)\n",
    " '''\n",
    "\n",
    "X = np.array([[0.5,0.1,4.9,4.9,0.5],\n",
    "              [3.1,0.43,47.5,0.53,8.7],\n",
    "              [11,3,14,1,1]])\n",
    "'''\n",
    "  [8.7,2300,30],\n",
    "  [8.7,2300,36],\n",
    "  [8.6,2300,20],\n",
    "  [7.1,1400,16],  \n",
    " [8.8,2100,32],\n",
    "  [8.7,2000,25],\n",
    "  [7.7,1500,20],   \n",
    "  [9,3300,15],\n",
    "  [8.1,1800,22],\n",
    "  [8.4,2200,25],\n",
    "  [8.9,2300,30],   \n",
    "  [8.5,2000,22],\n",
    "  [7.9,1900,18],\n",
    "  [8,2300,16],\n",
    "  [8.1,2200,16],\n",
    "  [8.6,2500,18],\n",
    "  [8.3,2400,15],\n",
    "   [8.5,2200,15],\n",
    "    [8.9,1000,18],\n",
    "    [8.1,1000,18],\n",
    "    [7.7,1500,20],\n",
    "    [7.6,1700,18]])\n",
    "'''\n",
    "X_norm = zscore_normalize_features(X)\n",
    "y = np.array([0,1,0])   #,140,185,171,150,200 ,220 ,250,130,200,185,195,210,250,230,220,160,150,180,250,220,230,210]) \n",
    "   # it does matter whether the target data is sorted or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0df627c-ceae-4a13-853e-a55d956f1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, compute_cost_logistic_regression, compute_gradient, alpha, iterations):\n",
    "    '''\n",
    "    Descriptions: performs batch gradient descent to learn w vector and b parameter, using learning rate alpha and number of iterations\n",
    "    Preconditions: X - m x n matrix which contains our data \n",
    "                    y - m size array - target values\n",
    "                    w - n size array - model parameters\n",
    "                    b - real number - free term parameter\n",
    "                    compute_cost_linear_regression, compute_gradient : functions to compute cost & gradient\n",
    "                    alpha : learning rate\n",
    "                    iterations: number of iterations\n",
    "    Postconditons: w , b - updated values of parameters of the model & hist - array to store cost and w at every iteration\n",
    "    '''\n",
    "    hist = []\n",
    "    for i in range(iterations):\n",
    "        # calculate gradient for our parameters\n",
    "        dj_db, dj_dw = compute_gradient(X, y, w, b,0.7)\n",
    "\n",
    "        # update parameters using the learning rate\n",
    "        w -= alpha* dj_dw\n",
    "        b -= alpha* dj_db\n",
    "\n",
    "        # save cost at every iteration if we do not have too many iterations (avoid resource exhaustion)\n",
    "        if i<100000:\n",
    "            hist.append(compute_cost_logistic_regression(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(iterations/ 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {hist[-1]:8.2f}   \")\n",
    "\n",
    "    return w, b, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d07fd62c-5c0b-447e-8212-ca5ee1ae4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.69   \n",
      "Iteration  100: Cost     0.41   \n",
      "Iteration  200: Cost     0.36   \n",
      "Iteration  300: Cost     0.34   \n",
      "Iteration  400: Cost     0.34   \n",
      "Iteration  500: Cost     0.34   \n",
      "Iteration  600: Cost     0.33   \n",
      "Iteration  700: Cost     0.33   \n",
      "Iteration  800: Cost     0.33   \n",
      "Iteration  900: Cost     0.33   \n",
      "w,b found by gradient descent: [-0.1874327  -0.26692465  0.61240596 -0.355787    0.62634252] -0.7193849350901343\n"
     ]
    }
   ],
   "source": [
    "w_in= np.zeros((X_norm.shape[1],))\n",
    "b_in= 0\n",
    "\n",
    "iterations=1000\n",
    "alpha=0.01\n",
    "\n",
    "\n",
    "w,b,cost_history=gradient_descent(X_norm, y, w_in, b_in, compute_cost_logistic_regression, compute_gradient, alpha, iterations)\n",
    "\n",
    "print(\"w,b found by gradient descent:\", w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ab50f-4e2a-4df1-be02-81c088750501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047640b9-b792-4d46-9b27-8d7337026b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
